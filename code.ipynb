{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7051863,"sourceType":"datasetVersion","datasetId":4058532},{"sourceId":7082446,"sourceType":"datasetVersion","datasetId":4080187}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Load basic libraries\nimport pandas as pd\nimport numpy as np\nfrom functools import partial\nimport re\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:25:35.823464Z","iopub.execute_input":"2023-11-28T22:25:35.824204Z","iopub.status.idle":"2023-11-28T22:25:35.830451Z","shell.execute_reply.started":"2023-11-28T22:25:35.824168Z","shell.execute_reply":"2023-11-28T22:25:35.828663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_excel(\"/kaggle/input/data-train/Data_Train.xlsx\")\ndata","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:25:35.837766Z","iopub.execute_input":"2023-11-28T22:25:35.838243Z","iopub.status.idle":"2023-11-28T22:25:37.985565Z","shell.execute_reply.started":"2023-11-28T22:25:35.838206Z","shell.execute_reply":"2023-11-28T22:25:37.984226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data.drop([\"Price\"],axis = 1), data[\"Price\"], test_size=0.15, random_state=123)\nX_test","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:25:37.988185Z","iopub.execute_input":"2023-11-28T22:25:37.988645Z","iopub.status.idle":"2023-11-28T22:25:38.015402Z","shell.execute_reply.started":"2023-11-28T22:25:37.988602Z","shell.execute_reply":"2023-11-28T22:25:38.014230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:25:38.016919Z","iopub.execute_input":"2023-11-28T22:25:38.017279Z","iopub.status.idle":"2023-11-28T22:25:38.036921Z","shell.execute_reply.started":"2023-11-28T22:25:38.017251Z","shell.execute_reply":"2023-11-28T22:25:38.035558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:25:38.039289Z","iopub.execute_input":"2023-11-28T22:25:38.040344Z","iopub.status.idle":"2023-11-28T22:25:38.054465Z","shell.execute_reply.started":"2023-11-28T22:25:38.040309Z","shell.execute_reply":"2023-11-28T22:25:38.053263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_train[\"Genre\"].unique())","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:25:38.055860Z","iopub.execute_input":"2023-11-28T22:25:38.056190Z","iopub.status.idle":"2023-11-28T22:25:38.071203Z","shell.execute_reply.started":"2023-11-28T22:25:38.056163Z","shell.execute_reply":"2023-11-28T22:25:38.070083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\n\n# Fit and transform the categorical data\nencoded_dataTrain = label_encoder.fit_transform(X_train[\"BookCategory\"])\nencoded_dataTest= label_encoder.transform(X_test[\"BookCategory\"])\nencoded_dataTrain","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:25:38.072421Z","iopub.execute_input":"2023-11-28T22:25:38.073198Z","iopub.status.idle":"2023-11-28T22:25:38.091235Z","shell.execute_reply.started":"2023-11-28T22:25:38.073166Z","shell.execute_reply":"2023-11-28T22:25:38.090019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Univariate Variable Analysis\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basic Data Analysis\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Feature Engineering\n\n","metadata":{}},{"cell_type":"code","source":"def getPattern(pattern ,text):\n    \n    match = re.search(pattern, text)\n    \n    if match:\n        return float(match.group())\n    else:\n        return None  # or any other value or indicator you want for cases where no match is found\n","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:25:38.092804Z","iopub.execute_input":"2023-11-28T22:25:38.093754Z","iopub.status.idle":"2023-11-28T22:25:38.103285Z","shell.execute_reply.started":"2023-11-28T22:25:38.093719Z","shell.execute_reply":"2023-11-28T22:25:38.102131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getPatternString(text):\n  # Define a regular expression pattern to match words within brackets\n    pattern = r'\\(([^)]+)\\)'\n\n    # Use re.findall to find all matches in the text\n    matches = re.findall(pattern, text)\n    for match in matches:\n        return match.split()[0]\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:25:38.105022Z","iopub.execute_input":"2023-11-28T22:25:38.105356Z","iopub.status.idle":"2023-11-28T22:25:38.116145Z","shell.execute_reply.started":"2023-11-28T22:25:38.105329Z","shell.execute_reply":"2023-11-28T22:25:38.115228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"R_pattern = r'\\b\\d+\\.\\d+\\b'\n\ngetReviews = partial(getPattern, R_pattern)\n\nX_train[\"Reviews\"] = X_train[\"Reviews\"].apply(getReviews)\nX_train[\"Reviews\"]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:25:38.117447Z","iopub.execute_input":"2023-11-28T22:25:38.118161Z","iopub.status.idle":"2023-11-28T22:25:38.149880Z","shell.execute_reply.started":"2023-11-28T22:25:38.118127Z","shell.execute_reply":"2023-11-28T22:25:38.148878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"year_pattern = r'\\b\\d{4}\\b'\n\ngetYear = partial(getPattern, year_pattern)\n\nX_train[\"Year\"] = X_train[\"Edition\"].apply(getYear)\nX_train[\"Year\"]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:25:38.154126Z","iopub.execute_input":"2023-11-28T22:25:38.154536Z","iopub.status.idle":"2023-11-28T22:25:38.185210Z","shell.execute_reply.started":"2023-11-28T22:25:38.154500Z","shell.execute_reply":"2023-11-28T22:25:38.184043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"number_pattern = r'\\b\\d+\\b'\n\ngetNumber = partial(getPattern, number_pattern)\n\nX_train[\"Ratings\"] = X_train[\"Ratings\"].apply(getNumber)\nX_train[\"Ratings\"]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:25:38.186831Z","iopub.execute_input":"2023-11-28T22:25:38.187300Z","iopub.status.idle":"2023-11-28T22:25:38.219446Z","shell.execute_reply.started":"2023-11-28T22:25:38.187270Z","shell.execute_reply":"2023-11-28T22:25:38.218633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nX_train[\"Type\"] = X_train[\"Genre\"].apply(getPatternString)\n\nlabel_encoder = LabelEncoder()\n\nX_train[\"Type\"] = label_encoder.fit_transform(X_train[\"Type\"])\n\n(X_train[\"Type\"].unique())","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:25:38.220548Z","iopub.execute_input":"2023-11-28T22:25:38.221248Z","iopub.status.idle":"2023-11-28T22:25:38.248018Z","shell.execute_reply.started":"2023-11-28T22:25:38.221216Z","shell.execute_reply":"2023-11-28T22:25:38.246887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_encoder2 = LabelEncoder()\n\nX_train[\"BookCategory\"] = label_encoder2.fit_transform(X_train[\"BookCategory\"])\n\nX_train[\"BookCategory\"].unique()","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:25:38.249250Z","iopub.execute_input":"2023-11-28T22:25:38.250057Z","iopub.status.idle":"2023-11-28T22:25:38.260283Z","shell.execute_reply.started":"2023-11-28T22:25:38.250026Z","shell.execute_reply":"2023-11-28T22:25:38.259058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:25:38.262105Z","iopub.execute_input":"2023-11-28T22:25:38.262829Z","iopub.status.idle":"2023-11-28T22:25:38.286130Z","shell.execute_reply.started":"2023-11-28T22:25:38.262785Z","shell.execute_reply":"2023-11-28T22:25:38.285332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\ndef getType(text):\n    words = re.findall(r'\\b\\w+\\b', text)\n    return next(word for word in words if words.count(word) == 1)\n\n\n\n#X_train[\"Type2\"] = X_train[\"Edition\"].apply(getType)\n\n#X_train[\"Type2\"]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:25:38.287449Z","iopub.execute_input":"2023-11-28T22:25:38.287793Z","iopub.status.idle":"2023-11-28T22:25:38.294139Z","shell.execute_reply.started":"2023-11-28T22:25:38.287763Z","shell.execute_reply":"2023-11-28T22:25:38.292749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_encoder3 = LabelEncoder()\n\n#X_train[\"Type2\"] = label_encoder3.fit_transform(X_train[\"Type2\"])\n\n#X_train","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:25:38.295326Z","iopub.execute_input":"2023-11-28T22:25:38.295903Z","iopub.status.idle":"2023-11-28T22:25:38.304955Z","shell.execute_reply.started":"2023-11-28T22:25:38.295871Z","shell.execute_reply":"2023-11-28T22:25:38.303769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature transformation","metadata":{}},{"cell_type":"code","source":"X_train['TAE'] = X_train['Title']+ ' ' + X_train['Author'] + ' ' + X_train['Edition']\nX_train['SG'] =  X_train['Genre']+ ' ' + X_train['Synopsis']+ ' ' + X_train['Genre']\n\nX_train[\"SG\"][0]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:25:38.306494Z","iopub.execute_input":"2023-11-28T22:25:38.306853Z","iopub.status.idle":"2023-11-28T22:25:38.343554Z","shell.execute_reply.started":"2023-11-28T22:25:38.306826Z","shell.execute_reply":"2023-11-28T22:25:38.342650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\ntfidf_vectorizerTAE = TfidfVectorizer()\n\ntfidf_matrix = tfidf_vectorizerTAE.fit_transform(X_train['TAE'])\n\ndense_matrix = tfidf_matrix.todense()\n\nTAE = pd.DataFrame(dense_matrix)\n\nnum_components = 16  # You can change this based on your requirement\npca = PCA(n_components=num_components)\nprincipal_components = pca.fit_transform(TAE)\n\npc_columns = [f'TAE-PC_{i+1}' for i in range(num_components)]\n\npc_df = pd.DataFrame(data=principal_components, columns=pc_columns)\n\n# Concatenate the original DataFrame and the PCA DataFrame\n\npc_df","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:25:38.344989Z","iopub.execute_input":"2023-11-28T22:25:38.345349Z","iopub.status.idle":"2023-11-28T22:25:44.261025Z","shell.execute_reply.started":"2023-11-28T22:25:38.345319Z","shell.execute_reply":"2023-11-28T22:25:44.259907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Set the number of clusters for hierarchical clustering\nn_clusters = 20  # You can adjust this based on your requirement\n\n# Apply hierarchical clustering on the principal components\nhierarchical_clustering = AgglomerativeClustering(n_clusters=n_clusters)\nhierarchical_clusters = hierarchical_clustering.fit_predict(pc_df)\n\npc_df['ClusterTAE'] = hierarchical_clusters\n\ncluster_counts = pc_df['ClusterTAE'].value_counts()\n\npc_df","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:25:44.262579Z","iopub.execute_input":"2023-11-28T22:25:44.263105Z","iopub.status.idle":"2023-11-28T22:25:45.255673Z","shell.execute_reply.started":"2023-11-28T22:25:44.263066Z","shell.execute_reply":"2023-11-28T22:25:45.254549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\ntfidf_vectorizerSG = TfidfVectorizer()\n\ntfidf_matrix = tfidf_vectorizerSG.fit_transform(X_train['SG'])\n\ndense_matrix = tfidf_matrix.todense()\n\nSG = pd.DataFrame(dense_matrix)\n\nnum_components = 16  # You can change this based on your requirement\npcaSG = PCA(n_components=num_components)\nprincipal_components = pca.fit_transform(SG)\n\npc_columns = [f'SG-PC_{i+1}' for i in range(num_components)]\n\nSGDF = pd.DataFrame(data=principal_components, columns=pc_columns)\n\n# Concatenate the original DataFrame and the PCA DataFrame\n\nSGDF","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:25:45.257249Z","iopub.execute_input":"2023-11-28T22:25:45.258436Z","iopub.status.idle":"2023-11-28T22:26:01.053757Z","shell.execute_reply.started":"2023-11-28T22:25:45.258391Z","shell.execute_reply":"2023-11-28T22:26:01.052568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Set the number of clusters for hierarchical clustering\nn_clusters = 20  # You can adjust this based on your requirement\n\n# Apply hierarchical clustering on the principal components\nhierarchical_clusteringSG = AgglomerativeClustering(n_clusters=n_clusters)\nhierarchical_clusters = hierarchical_clusteringSG.fit_predict(SGDF)\n\nSGDF['ClusterSG'] = hierarchical_clusters\nSGDF","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:26:01.055429Z","iopub.execute_input":"2023-11-28T22:26:01.055939Z","iopub.status.idle":"2023-11-28T22:26:02.085111Z","shell.execute_reply.started":"2023-11-28T22:26:01.055899Z","shell.execute_reply":"2023-11-28T22:26:02.083951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:26:02.086677Z","iopub.execute_input":"2023-11-28T22:26:02.087012Z","iopub.status.idle":"2023-11-28T22:26:02.113202Z","shell.execute_reply.started":"2023-11-28T22:26:02.086982Z","shell.execute_reply":"2023-11-28T22:26:02.112067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.reset_index(drop=True, inplace=True)\nX_train","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:26:02.115186Z","iopub.execute_input":"2023-11-28T22:26:02.115633Z","iopub.status.idle":"2023-11-28T22:26:02.144273Z","shell.execute_reply.started":"2023-11-28T22:26:02.115592Z","shell.execute_reply":"2023-11-28T22:26:02.143148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lastX_train = pd.concat([pc_df, SGDF], axis=1)\n#lastX_train[\"Type2\"] = X_train[\"Type2\"]\nlastX_train[\"Type\"] = X_train[\"Type\"]\nlastX_train[\"Year\"] = X_train[\"Year\"]\nlastX_train[\"Ratings\"] = X_train[\"Ratings\"]\nlastX_train[\"Reviews\"] = X_train[\"Reviews\"]\nlastX_train[\"BookCategory\"] = X_train[\"BookCategory\"]\nlastX_train","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:26:17.979344Z","iopub.execute_input":"2023-11-28T22:26:17.979845Z","iopub.status.idle":"2023-11-28T22:26:18.023469Z","shell.execute_reply.started":"2023-11-28T22:26:17.979794Z","shell.execute_reply":"2023-11-28T22:26:18.022123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lastX_train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:26:20.921233Z","iopub.execute_input":"2023-11-28T22:26:20.921661Z","iopub.status.idle":"2023-11-28T22:26:20.933770Z","shell.execute_reply.started":"2023-11-28T22:26:20.921629Z","shell.execute_reply":"2023-11-28T22:26:20.932444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lastX_train = lastX_train.dropna()\nlastX_train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:26:02.276879Z","iopub.status.idle":"2023-11-28T22:26:02.277438Z","shell.execute_reply.started":"2023-11-28T22:26:02.277220Z","shell.execute_reply":"2023-11-28T22:26:02.277240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:26:23.466561Z","iopub.execute_input":"2023-11-28T22:26:23.467018Z","iopub.status.idle":"2023-11-28T22:26:23.489212Z","shell.execute_reply.started":"2023-11-28T22:26:23.466985Z","shell.execute_reply":"2023-11-28T22:26:23.487927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tempTest = X_test\n\ngetNumber = partial(getPattern, number_pattern)\ngetReviews = partial(getPattern, R_pattern)\ngetYear = partial(getPattern, year_pattern)\n\nX_test[\"Reviews\"] = X_test[\"Reviews\"].apply(getReviews)\nX_test[\"Year\"] = X_test[\"Edition\"].apply(getYear)\nX_test[\"Ratings\"] = X_test[\"Ratings\"].apply(getNumber)\n#X_test[\"Type2\"] = X_test[\"Edition\"].apply(getType)\nX_test[\"Type\"] = X_test[\"Genre\"].apply(getPatternString)\n#X_test[\"Type2\"] = label_encoder3.transform(X_test[\"Type2\"])\nX_test[\"BookCategory\"] = label_encoder2.transform(X_test[\"BookCategory\"])\nX_test[\"Type\"] = label_encoder.transform(X_test[\"Type\"])\nX_test","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:26:26.871029Z","iopub.execute_input":"2023-11-28T22:26:26.871496Z","iopub.status.idle":"2023-11-28T22:26:26.923012Z","shell.execute_reply.started":"2023-11-28T22:26:26.871463Z","shell.execute_reply":"2023-11-28T22:26:26.921761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test['TAE'] = X_test['Title']+ ' ' + X_test['Author'] + ' ' + X_test['Edition']\nX_test['SG'] =  X_test['Genre']+ ' ' + X_test['Synopsis']+ ' ' + X_test['Genre']\n\nX_test","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:27:51.090020Z","iopub.execute_input":"2023-11-28T22:27:51.090424Z","iopub.status.idle":"2023-11-28T22:27:51.128752Z","shell.execute_reply.started":"2023-11-28T22:27:51.090391Z","shell.execute_reply":"2023-11-28T22:27:51.127061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_matrix = tfidf_vectorizerTAE.fit_transform(X_test['TAE'])\n\ndense_matrix = tfidf_matrix.todense()\n\nTAE = pd.DataFrame(dense_matrix)\n\nnum_components = 16  # You can change this based on your requirement\npca = PCA(n_components=num_components)\nprincipal_components = pca.fit_transform(TAE)\n\npc_columns = [f'TAE-PC_{i+1}' for i in range(num_components)]\n\npc_df = pd.DataFrame(data=principal_components, columns=pc_columns)\n\n# Concatenate the original DataFrame and the PCA DataFrame\n\npc_df","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:28:32.167848Z","iopub.execute_input":"2023-11-28T22:28:32.168275Z","iopub.status.idle":"2023-11-28T22:28:32.871694Z","shell.execute_reply.started":"2023-11-28T22:28:32.168244Z","shell.execute_reply":"2023-11-28T22:28:32.870330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hierarchical_clusters = hierarchical_clustering.fit_predict(pc_df)\n\npc_df['ClusterTAE'] = hierarchical_clusters\n\ncluster_counts = pc_df['ClusterTAE'].value_counts()\n\npc_df","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:29:07.137452Z","iopub.execute_input":"2023-11-28T22:29:07.138055Z","iopub.status.idle":"2023-11-28T22:29:07.194162Z","shell.execute_reply.started":"2023-11-28T22:29:07.138013Z","shell.execute_reply":"2023-11-28T22:29:07.192918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_matrix = tfidf_vectorizerSG.fit_transform(X_train['SG'])\n\ndense_matrix = tfidf_matrix.todense()\n\nSG = pd.DataFrame(dense_matrix)\n\nnum_components = 16  # You can change this based on your requirement\npcaSG = PCA(n_components=num_components)\nprincipal_components = pca.fit_transform(SG)\n\npc_columns = [f'SG-PC_{i+1}' for i in range(num_components)]\n\nSGDF = pd.DataFrame(data=principal_components, columns=pc_columns)\n\n# Concatenate the original DataFrame and the PCA DataFrame\n\nSGDF","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"markdown","source":"Do not change this part of the code only run it!","metadata":{}},{"cell_type":"code","source":"def train(X_train, y_train, X_test, y_test):\n    \"\"\"\n    Trains a RandomForestRegressor model and evaluates its performance using the mean squared error (MSE).\n\n    Parameters:\n    X (numpy.ndarray or pandas.DataFrame): The training data with (n_rows, n_features) shape.\n    y (numpy.ndarray or pandas.Series): The target variable (n_rows, 1) shape.\n\n    Returns:\n    float: The mean squared error (MSE) of the predictions(train data) made by the RandomForestRegressor.\n    float: The mean squared error (MSE) of the predictions(test data) made by the RandomForestRegressor.\n    \"\"\"\n    random_forest_regressor = RandomForestRegressor(criterion='squared_error')\n    random_forest_regressor.fit(X_train, y_train)\n    \n    y_train_pred = random_forest_regressor.predict(X_train)\n    y_test_pred = random_forest_regressor.predict(X_test)\n    \n    mse_train = random_forest_regressor.score(X_train, y_train)\n    mse_test = mean_squared_error(y_test, y_test_pred)\n    \n    r2_train = r2_score(y_train, y_train_pred)\n    r2_test = r2_score(y_test, y_test_pred)\n\n    #y_pred = random_forest_regressor.predict(X_test)\n\n    return mse_train, mse_test, r2_train ,r2_test","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:26:02.283610Z","iopub.status.idle":"2023-11-28T22:26:02.284171Z","shell.execute_reply.started":"2023-11-28T22:26:02.283898Z","shell.execute_reply":"2023-11-28T22:26:02.283924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mse_train, mse_test, r2_train ,r2_test = train(X_train=encoded_dataTrain.reshape(-1, 1) , y_train=y_train , X_test=encoded_dataTest.reshape(-1, 1) , y_test=y_test)\nprint(\"Train mse is: {} // Test mse is: {} // Train R2 is: {} // Test R2 is: {}\".format(mse_train,mse_test,r2_train ,r2_test))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:26:02.285939Z","iopub.status.idle":"2023-11-28T22:26:02.286712Z","shell.execute_reply.started":"2023-11-28T22:26:02.286486Z","shell.execute_reply":"2023-11-28T22:26:02.286526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_forest_regressor = RandomForestRegressor(criterion='squared_error')\nrandom_forest_regressor.fit(encoded_data.reshape(-1, 1) ,y_train)\nmse_train = random_forest_regressor.score(encoded_data.reshape(-1, 1) ,y_train)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-28T22:26:02.287784Z","iopub.status.idle":"2023-11-28T22:26:02.288173Z","shell.execute_reply.started":"2023-11-28T22:26:02.287989Z","shell.execute_reply":"2023-11-28T22:26:02.288007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Post processing","metadata":{}},{"cell_type":"markdown","source":"Error Analysis: Analyzing the model's errors on the test data can provide valuable insights into its strengths and weaknesses. This involves identifying common error patterns, understanding the causes of errors, and exploring strategies to mitigate them.\n\nFeature Importance Analysis: Identifying the most important features contributing to the model's predictions can help in feature selection and dimensionality reduction. This can lead to a more efficient and interpretable preprocessing pipeline.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}